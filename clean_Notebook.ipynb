{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMuYmPleIbr"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6u2a6GKR_o2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #Setup: Imports and dependencies\n",
        "#@markdown Install and import Python dependencies.\n",
        "location = 'vsc' #@param['colab', 'vsc']\n",
        "\n",
        "package = 'inseq' #@param['inseq', 'kayo', 'both']\n",
        "\n",
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import json\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import copy\n",
        "import sys\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import gc\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "if package == 'inseq' or package == 'both':\n",
        "  print('Installing dependencies inseq')\n",
        "  !pip install inseq\n",
        "  import inseq\n",
        "\n",
        "\n",
        "if package == 'kayo' or package == 'both':\n",
        "  print('Installing dependencies kayo')\n",
        "  !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "  !git clone https://github.com/JanAthmer/Baseline_clusters.git &> /dev/null\n",
        "#   !git clone https://github.com/kayoyin/interpret-lm.git &> /dev/null\n",
        "  sys.path.append('./Baseline_clusters')\n",
        "#   sys.path.append('./interpret-lm')\n",
        "\n",
        "  from lm_saliency import *\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JkZAqsOKhCsT"
      },
      "outputs": [],
      "source": [
        "#@title #Setup: load Drive\n",
        "if location == 'colab':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQa4aaAA27B1"
      },
      "outputs": [],
      "source": [
        "#@title #Setup: Utils\n",
        "\n",
        "def average(lst):\n",
        "  return sum(lst)/len(lst)\n",
        "\n",
        "\n",
        "def reciprocal_rank(predictions, targets):\n",
        "    # Combine predictions and targets into pairs\n",
        "    pairs = list(zip(predictions, targets))\n",
        "\n",
        "    # Sort the pairs based on the prediction values (in descending order)\n",
        "    sorted_pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Find the rank of the first correct prediction\n",
        "    rank = next((i + 1 for i, (pred, target) in enumerate(sorted_pairs) if target), 0)\n",
        "\n",
        "    # Calculate reciprocal rank\n",
        "    reciprocal_rank = 1 / rank if rank > 0 else 0\n",
        "\n",
        "    return reciprocal_rank\n",
        "\n",
        "def get_target(sentence):\n",
        "    b = sentence.split(' ')\n",
        "    prefix_l = 0\n",
        "    postix_l = 0\n",
        "    target_l = 0\n",
        "    check = False\n",
        "    for i,word in enumerate(b):\n",
        "        if i != 0:\n",
        "            word = \" \"+word\n",
        "\n",
        "        if check == True:\n",
        "            postix_l += len(tokenizer(word)['input_ids'])\n",
        "        if \"n't\" in word:\n",
        "            target_l = len(tokenizer(word)['input_ids'])\n",
        "            check = True\n",
        "        if check == False:\n",
        "            prefix_l += len(tokenizer(word)['input_ids'])\n",
        "    return([False]*prefix_l+[True]*target_l+[False]*postix_l)\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_one_hot_list(length):\n",
        "    # Check if the length is valid\n",
        "    if length <= 0:\n",
        "        raise ValueError(\"Length must be greater than 0\")\n",
        "\n",
        "    # Choose a random index for the \"hot\" element\n",
        "    hot_index = random.randint(0, length - 1)\n",
        "\n",
        "    # Create the one-hot list\n",
        "    one_hot_list = [0] * length\n",
        "    one_hot_list[hot_index] = 1\n",
        "\n",
        "    return one_hot_list\n",
        "\n",
        "def get_last_true_index(one_hot_vector):\n",
        "    \"\"\"\n",
        "    Get the index of the last True value in the one-hot vector.\n",
        "\n",
        "    Parameters:\n",
        "    one_hot_vector (list of bool): One-hot vector with possibly multiple True values.\n",
        "\n",
        "    Returns:\n",
        "    int: The index of the last True value in the one-hot vector, or -1 if no True values are found.\n",
        "    \"\"\"\n",
        "    last_true_index = -1\n",
        "    for i, value in enumerate(one_hot_vector):\n",
        "        if value:\n",
        "            last_true_index = i\n",
        "    return last_true_index\n",
        "\n",
        "def normalize_vector(vector):\n",
        "    \"\"\"\n",
        "    Normalize a vector between -1 and 1 by dividing each element by the absolute maximum value.\n",
        "\n",
        "    Args:\n",
        "    vector (numpy array): The input vector to be normalized.\n",
        "\n",
        "    Returns:\n",
        "    numpy array: The normalized vector.\n",
        "    \"\"\"\n",
        "    abs_max = max(abs(np.max(vector)), abs(np.min(vector)))\n",
        "    normalized_vector = vector / abs_max\n",
        "    return normalized_vector\n",
        "\n",
        "\n",
        "def normalize_and_truncate(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    Normalize two input vectors between -1 and 1 and truncate the longer vector to match the length of the shorter one.\n",
        "\n",
        "    Args:\n",
        "    vector_a (numpy array): The first input vector to be normalized and truncated.\n",
        "    vector_b (numpy array): The second input vector to be normalized and truncated.\n",
        "\n",
        "    Returns:\n",
        "    numpy array, numpy array: The normalized and truncated vectors A and B.\n",
        "    \"\"\"\n",
        "    # Normalize vector A\n",
        "    abs_max_a = max(abs(np.max(vector_a)), abs(np.min(vector_a)))\n",
        "    normalized_vector_a = vector_a / abs_max_a\n",
        "\n",
        "    # Normalize vector B\n",
        "    abs_max_b = max(abs(np.max(vector_b)), abs(np.min(vector_b)))\n",
        "    normalized_vector_b = vector_b / abs_max_b\n",
        "\n",
        "    # Determine which vector is longer and truncate it\n",
        "    if len(normalized_vector_a) > len(normalized_vector_b):\n",
        "        truncated_vector_a = normalized_vector_a[:len(normalized_vector_b)]\n",
        "        truncated_vector_b = normalized_vector_b\n",
        "    else:\n",
        "        truncated_vector_a = normalized_vector_a\n",
        "        truncated_vector_b = normalized_vector_b[:len(normalized_vector_a)]\n",
        "\n",
        "    return truncated_vector_a, truncated_vector_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "B5RR_sg4mCjV"
      },
      "outputs": [],
      "source": [
        "#@title #Setup: Load Data\n",
        "with jsonlines.open(\"npi_present_1.jsonl\", 'r') as f:\n",
        "  npi = pd.DataFrame(f)\n",
        "\n",
        "with jsonlines.open(\"determiner_noun_agreement_1.jsonl\", 'r') as f:\n",
        "  dna = pd.DataFrame(f)\n",
        "\n",
        "sentences = []\n",
        "values = []\n",
        "with open(\"prefix+value.tsv\", 'r', encoding='utf-8') as ifh:\n",
        "  for line in ifh:\n",
        "      sentence, value = line.strip().split('\\t')\n",
        "      sentences.append(sentence)\n",
        "      values.append(value)\n",
        "  any = pd.DataFrame({'one_prefix_prefix': sentences})\n",
        "\n",
        "with open(\"vocab_tagged.json\", 'r') as file:\n",
        "    vocabulary = json.load(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main function and loop**"
      ],
      "metadata": {
        "id": "KJPKKPchqpav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j6ENx7scmCjV"
      },
      "outputs": [],
      "source": [
        "#@title #Main Function\n",
        "def rr_attribution(model_inseq, attributed_fn, sentence, target, targets, contrastive = True, foil = None, baseline = True, rnd = None):\n",
        "\n",
        "    if attributed_fn == 'logit':\n",
        "        contrast_attributed_fn = \"contrast_logits_diff\"\n",
        "    elif attributed_fn == 'probability':\n",
        "        contrast_attributed_fn = \"contrast_prob_diff\"\n",
        "\n",
        "    if baseline:\n",
        "        base = model_inseq.attribute(\n",
        "                  sentence,\n",
        "                  sentence + \" \" + target,\n",
        "                  attributed_fn= attributed_fn,\n",
        "                )\n",
        "\n",
        "        base_att = base[0].target_attributions\n",
        "        if explanation != 'occlusion':\n",
        "            base_att = base_att.sum(axis = 2)\n",
        "        base_att = torch.flatten(base_att[~torch.any(base_att.isnan(),dim=1)]).numpy()\n",
        "        base_att_rev = base_att*-1\n",
        "\n",
        "        base_rank = reciprocal_rank(base_att, targets)\n",
        "        base_rank_rev = reciprocal_rank(base_att_rev, targets)\n",
        "\n",
        "    if contrastive:\n",
        "        con = model_inseq.attribute(\n",
        "          sentence,\n",
        "          sentence + \" \" + target,\n",
        "          attributed_fn= contrast_attributed_fn,\n",
        "          contrast_targets= sentence + \" \" + foil,\n",
        "          step_scores=[contrast_attributed_fn]\n",
        "        )\n",
        "        con_att = con[0].target_attributions\n",
        "        if explanation != 'occlusion':\n",
        "            con_att = con_att.sum(axis = 2)\n",
        "        con_att = torch.flatten(con_att[~torch.any(con_att.isnan(),dim=1)]).numpy()\n",
        "        con_att_rev = con_att*-1\n",
        "\n",
        "        con_rank = reciprocal_rank(con_att, targets)\n",
        "        con_rank_rev = reciprocal_rank(con_att_rev, targets)\n",
        "\n",
        "    if rnd != None:\n",
        "        con_rnd = model_inseq.attribute(\n",
        "          sentence,\n",
        "          sentence + \" \" + target,\n",
        "          attributed_fn= contrast_attributed_fn,\n",
        "          contrast_targets= sentence + \" \" + rnd,\n",
        "          step_scores=[contrast_attributed_fn]\n",
        "        )\n",
        "        con_rnd_att = con_rnd[0].target_attributions\n",
        "        if explanation != 'occlusion':\n",
        "            con_rnd_att = con_rnd_att.sum(axis = 2)\n",
        "        con_rnd_att = torch.flatten(con_rnd_att[~torch.any(con_rnd_att.isnan(),dim=1)]).numpy()\n",
        "        con_rnd_att_rev = con_rnd_att*-1\n",
        "\n",
        "        con_rnd_rank = reciprocal_rank(con_rnd_att, targets)\n",
        "        con_rnd_rank_rev = reciprocal_rank(con_rnd_att_rev, targets)\n",
        "\n",
        "    clear_output()\n",
        "    if baseline and contrastive and (rnd!=None):\n",
        "        return base_rank,base_rank_rev,con_rank,con_rank_rev,con_rnd_rank,con_rnd_rank_rev\n",
        "    elif baseline and contrastive:\n",
        "        return base_rank,base_rank_rev,con_rank,con_rank_rev\n",
        "    elif baseline:\n",
        "        return base_rank,base_rank_rev\n",
        "    elif contrastive:\n",
        "        return con_rank,con_rank_rev\n",
        "    elif (rnd!=None):\n",
        "        return con_rnd_rank,con_rnd_rank_rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjQMc5tXmCjX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #Main Loop\n",
        "results = []\n",
        "\n",
        "explanations    = [\"input_x_gradient\",  \"lime\", \"occlusion\"]\n",
        "datasets        = [\"npi\", \"dna\", \"any\"]\n",
        "attributed_fns  = [\"logit\",\"probability\"]\n",
        "methods         = [\"base\", \"contrastive\", \"contrastive_rnd\"]\n",
        "reversed        = [\"Original\", \"Reversed\"]\n",
        "\n",
        "for explanation in explanations:\n",
        "  model_inseq = inseq.load_model(\"gpt2\", explanation)\n",
        "\n",
        "  for data in datasets:\n",
        "\n",
        "    for attributed_fn in attributed_fns:\n",
        "\n",
        "      for index, row in data.iterrows():\n",
        "\n",
        "            if data == 'any':\n",
        "              sentence = row[\"one_prefix_prefix\"]\n",
        "              target = \"any\"\n",
        "              foil = \"some\"\n",
        "              targets = get_target(sentence)\n",
        "            elif data == 'npi':\n",
        "              sentence = row[\"one_prefix_prefix\"]\n",
        "              target = row[\"one_prefix_word_good\"]\n",
        "              foil = row[\"one_prefix_word_bad\"]\n",
        "              sent_len = len(sentence.split(' '))\n",
        "              targets = (sent_len-1)*[False] + [True]\n",
        "            elif data == 'dna':\n",
        "              sentence = row[\"one_prefix_prefix\"]\n",
        "              target = row[\"one_prefix_word_good\"]\n",
        "              foil = row[\"one_prefix_word_bad\"]\n",
        "              sent_len = len(sentence.split(' '))\n",
        "              targets =  [True]+(sent_len-1)*[False]\n",
        "\n",
        "            foil_rnd = random.sample(vocabulary,1)[0][0][0]\n",
        "\n",
        "            base_rr, base_rev_rr,con_rr, con_rev_rr, con_rnd_rr, con_rnd_rev_rr = rr_attribution(\n",
        "                model_inseq,attributed_fn,sentence, target,targets, rnd = foil_rnd)\n",
        "\n",
        "            results.append(base_rr)\n",
        "            results.append(base_rev_rr)\n",
        "\n",
        "            results.append(con_rr)\n",
        "            results.append(con_rev_rr)\n",
        "\n",
        "            results.append(con_rnd_rr)\n",
        "            results.append(con_rnd_rev_rr)\n",
        "\n",
        "# Create a MultiIndex from the dimensions\n",
        "multi_index = pd.MultiIndex.from_product([explanations, datasets, attributed_fns,methods,reversed], names=['explanations', 'datasets', 'attributed_fns', 'methods', 'reversed'])\n",
        "\n",
        "# Create the DataFrame using the MultiIndex and data\n",
        "df = pd.DataFrame(results, index=multi_index, columns=['Value'])\n",
        "df.to_csv('full_results.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}